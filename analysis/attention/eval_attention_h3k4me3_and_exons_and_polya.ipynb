{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dbf2734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 16:40:42.677183: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-08 16:40:47.901327: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-08 16:40:47.901484: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-08 16:40:47.901498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import basenji\n",
    "from basenji import bed\n",
    "from basenji import dataset\n",
    "from basenji import seqnn\n",
    "from basenji import trainer\n",
    "from basenji import gene as bgene\n",
    "from basenji import dna_io\n",
    "\n",
    "import json\n",
    "\n",
    "import pyranges as pr\n",
    "\n",
    "import kipoiseq\n",
    "from kipoiseq import Interval\n",
    "import pyfaidx\n",
    "\n",
    "import pysam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5fbf3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read model parameters\n",
    "\n",
    "params_file = \"params_pred.json\"\n",
    "\n",
    "with open(params_file) as params_open :\n",
    "    params = json.load(params_open)\n",
    "    \n",
    "    params_model = params['model']\n",
    "    params_train = params['train']\n",
    "\n",
    "#Read targets (GTEx RNA-seq)\n",
    "targets_file = '../bench_apa/targets_gtex.txt'\n",
    "\n",
    "targets_df = pd.read_csv(targets_file, index_col=0, sep='\\t')\n",
    "target_index = targets_df.index\n",
    "\n",
    "#Model ensemble parameters\n",
    "n_folds = 1\n",
    "SEQUENCE_LENGTH = 524288\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f010781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(exons_pr) = 291246\n",
      "len(gene_pr) = 1077833\n",
      "len(polya_pr) = 92482\n",
      "[Final] len(gene_pr) = 1170315\n"
     ]
    }
   ],
   "source": [
    "#Get exon annotation as dataframe\n",
    "exons_gff_file = '/home/drk/seqnn/analysis/2022/10-19/gencode41_basic_nort_protein_exons.gff'\n",
    "\n",
    "exons_pr = pr.read_gtf(exons_gff_file)\n",
    "\n",
    "exons_pr = exons_pr[exons_pr.Feature == 'exonic_part']\n",
    "exons_pr = exons_pr.drop('Source')\n",
    "exons_pr = exons_pr.drop('Score')\n",
    "\n",
    "print(\"len(exons_pr) = \" + str(len(exons_pr)))\n",
    "\n",
    "#Get basic gene annotation as dataframe\n",
    "gene_gff_file = './gencode.v41.basic.annotation.gff3'\n",
    "\n",
    "gene_pr = pr.read_gff3(gene_gff_file)\n",
    "\n",
    "gene_pr = gene_pr[gene_pr.Feature.isin(['gene', 'exon', 'five_prime_UTR', 'three_prime_UTR'])]\n",
    "\n",
    "gene_pr = gene_pr.drop('Source')\n",
    "gene_pr = gene_pr.drop('Score')\n",
    "gene_pr = gene_pr.drop('Frame')\n",
    "gene_pr = gene_pr.drop('ID')\n",
    "gene_pr = gene_pr.drop('hgnc_id')\n",
    "gene_pr = gene_pr.drop('havana_gene')\n",
    "gene_pr = gene_pr.drop('Parent')\n",
    "gene_pr = gene_pr.drop('transcript_support_level')\n",
    "gene_pr = gene_pr.drop('tag')\n",
    "gene_pr = gene_pr.drop('havana_transcript')\n",
    "gene_pr = gene_pr.drop('ont')\n",
    "gene_pr = gene_pr.drop('protein_id')\n",
    "gene_pr = gene_pr.drop('ccdsid')\n",
    "gene_pr = gene_pr.drop('artif_dupl')\n",
    "gene_pr = gene_pr.drop('level')\n",
    "\n",
    "print(\"len(gene_pr) = \" + str(len(gene_pr)))\n",
    "\n",
    "#Get pA annotation as dataframe\n",
    "polya_gff_file = './gencode.v41.polyAs.gff3'\n",
    "\n",
    "polya_pr = pr.read_gff3(polya_gff_file)\n",
    "\n",
    "polya_pr = polya_pr[polya_pr.Feature.isin(['polyA_signal', 'polyA_site'])]\n",
    "\n",
    "polya_pr = polya_pr.drop('Source')\n",
    "polya_pr = polya_pr.drop('Score')\n",
    "polya_pr = polya_pr.drop('Frame')\n",
    "polya_pr = polya_pr.drop('ID')\n",
    "polya_pr = polya_pr.drop('gene_type')\n",
    "polya_pr = polya_pr.drop('gene_name')\n",
    "polya_pr = polya_pr.drop('transcript_type')\n",
    "polya_pr = polya_pr.drop('transcript_name')\n",
    "polya_pr = polya_pr.drop('level')\n",
    "\n",
    "print(\"len(polya_pr) = \" + str(len(polya_pr)))\n",
    "\n",
    "#Concatenate annotations\n",
    "\n",
    "gene_pr = pr.concat([gene_pr, polya_pr])\n",
    "\n",
    "print(\"[Final] len(gene_pr) = \" + str(len(gene_pr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0409845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions / classes\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib.text import TextPath\n",
    "from matplotlib.patches import PathPatch, Rectangle\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "import gc\n",
    "\n",
    "#Function to get a one-hot coded sequence pattern\n",
    "def make_seq_1hot(genome_open, chrm, start, end, seq_len):\n",
    "    if start < 0:\n",
    "        seq_dna = 'N'*(-start) + genome_open.fetch(chrm, 0, end)\n",
    "    else:\n",
    "        seq_dna = genome_open.fetch(chrm, start, end)\n",
    "    \n",
    "    #Extend to full length (pad)\n",
    "    if len(seq_dna) < seq_len:\n",
    "        seq_dna += 'N'*(seq_len-len(seq_dna))\n",
    "    \n",
    "    seq_1hot = dna_io.dna_1hot(seq_dna)\n",
    "    \n",
    "    return seq_1hot\n",
    "\n",
    "#Function to inject into model definition to get attention scores\n",
    "def _get_attention_weights(self, inputs, training=False) :\n",
    "    \n",
    "    #Initialise the projection layers\n",
    "    embedding_size = self._value_size * self._num_heads\n",
    "    seq_len = inputs.shape[1]\n",
    "\n",
    "    #Compute q, k and v as multi-headed projections of the inputs\n",
    "    q = self._multihead_output(self._q_layer, inputs)  # [B, H, T, K]\n",
    "    k = self._multihead_output(self._k_layer, inputs)  # [B, H, T, K]\n",
    "    v = self._multihead_output(self._v_layer, inputs)  # [B, H, T, V]\n",
    "\n",
    "    #Scale the query by the square-root of key size\n",
    "    if self._scaling :\n",
    "        q *= self._key_size**-0.5\n",
    "\n",
    "    #[B, H, T', T]\n",
    "    content_logits = tf.matmul(q + self._r_w_bias, k, transpose_b=True)\n",
    "\n",
    "    if self._num_position_features == 0 :\n",
    "        logits = content_logits\n",
    "    else :\n",
    "        #Project positions to form relative keys.\n",
    "        distances = tf.range(-seq_len + 1, seq_len, dtype=tf.float32)[tf.newaxis]\n",
    "        positional_encodings = basenji.layers.positional_features(\n",
    "              positions=distances,\n",
    "              feature_size=self._num_position_features,\n",
    "              seq_length=seq_len,\n",
    "              symmetric=self._relative_position_symmetric)\n",
    "        #[1, 2T-1, Cr]\n",
    "      \n",
    "        if training :\n",
    "            positional_encodings = tf.nn.dropout(\n",
    "                positional_encodings, rate=self._positional_dropout_rate)\n",
    "\n",
    "        #[1, H, 2T-1, K]\n",
    "        r_k = self._multihead_output(self._r_k_layer, positional_encodings)\n",
    "\n",
    "        #Add shifted relative logits to content logits.\n",
    "        if self._content_position_bias :\n",
    "            #[B, H, T', 2T-1]\n",
    "            relative_logits = tf.matmul(q + self._r_r_bias, r_k, transpose_b=True)\n",
    "        else :\n",
    "            #[1, H, 1, 2T-1]\n",
    "            relative_logits = tf.matmul(self._r_r_bias, r_k, transpose_b=True)\n",
    "            #[1, H, T', 2T-1]\n",
    "            relative_logits = tf.broadcast_to(relative_logits, shape=(1, self._num_heads, seq_len, 2*seq_len-1))\n",
    "\n",
    "        #[B, H, T', T]\n",
    "        relative_logits = basenji.layers.relative_shift(relative_logits)\n",
    "        logits = content_logits + relative_logits\n",
    "\n",
    "    #Apply Softmax across length\n",
    "    weights = tf.nn.softmax(logits)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "#Function to create a new Keras model, outputting a specific attention layer's scores\n",
    "def get_attention_model(seqnn_model, layer_ix=0, inital_offset=30, offset=11) :\n",
    "\n",
    "    #Create new model object\n",
    "    attention_model = tf.keras.Model(\n",
    "        seqnn_model.model.layers[1].inputs,\n",
    "        _get_attention_weights(\n",
    "            seqnn_model.model.layers[1].layers[inital_offset + offset * layer_ix],\n",
    "            seqnn_model.model.layers[1].layers[inital_offset + offset * layer_ix - 1].output,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return attention_model\n",
    "\n",
    "#Function to predict tracks and attention scores\n",
    "def predict_tracks_and_attention_scores(models, sequence_one_hot, track_scale=1., track_transform=1., clip_soft=None, n_layers=8, score_rc=True) :\n",
    "    \n",
    "    n_folds = len(models)\n",
    "    \n",
    "    attention_scores = []\n",
    "    predicted_tracks = []\n",
    "    \n",
    "    #Loop over folds\n",
    "    for fold_ix in range(n_folds) :\n",
    "\n",
    "        attention_scores_for_fold = []\n",
    "\n",
    "        yh = models[fold_ix](sequence_one_hot[None, ...])[:, None, ...].astype('float32')\n",
    "        \n",
    "        #Undo scale\n",
    "        yh /= track_scale\n",
    "\n",
    "        #Undo soft_clip\n",
    "        if clip_soft is not None :\n",
    "            yh_unclipped = (yh - clip_soft)**2 + clip_soft\n",
    "            unclip_mask_h = (yh > clip_soft)\n",
    "\n",
    "            yh[unclip_mask_h] = yh_unclipped[unclip_mask_h]\n",
    "\n",
    "        #Undo sqrt\n",
    "        yh = yh**(1. / track_transform)\n",
    "\n",
    "        #Aggregate over tracks (average)\n",
    "        ##yh = np.mean(yh, axis=-1)\n",
    "\n",
    "        predicted_tracks.append(yh)\n",
    "\n",
    "        #Loop over layers\n",
    "        for layer_ix in range(n_layers) :\n",
    "\n",
    "            #Get attention score model and make predictions\n",
    "            attention_model = get_attention_model(models[fold_ix], layer_ix=layer_ix)\n",
    "            att_scores = attention_model.predict(x=[sequence_one_hot[None, ...]], batch_size=1)\n",
    "            \n",
    "            #Optionally reverse-complement\n",
    "            if score_rc :\n",
    "                att_scores_rc = attention_model.predict(x=[sequence_one_hot[None, ::-1, ::-1]], batch_size=1)\n",
    "                att_scores = (att_scores + att_scores_rc[..., ::-1, ::-1]) / 2.\n",
    "\n",
    "            attention_scores_for_fold.append(att_scores[:, None, None, ...])\n",
    "\n",
    "            attention_model = None\n",
    "            gc.collect()\n",
    "\n",
    "        #Concatenate\n",
    "        attention_scores_for_fold = np.concatenate(attention_scores_for_fold, axis=2)\n",
    "        attention_scores.append(attention_scores_for_fold)\n",
    "\n",
    "    #Concatenate\n",
    "    predicted_tracks = np.concatenate(predicted_tracks, axis=1)\n",
    "    attention_scores = np.concatenate(attention_scores, axis=1)\n",
    "\n",
    "    print(\"predicted_tracks.shape = \" + str(predicted_tracks.shape))\n",
    "    print(\"attention_scores.shape = \" + str(attention_scores.shape))\n",
    "    \n",
    "    return predicted_tracks, attention_scores\n",
    "\n",
    "#Function to predict tracks\n",
    "def predict_tracks(models, sequence_one_hot, track_scale=1., track_transform=1., clip_soft=None) :\n",
    "    \n",
    "    n_folds = len(models)\n",
    "    \n",
    "    predicted_tracks = []\n",
    "    \n",
    "    #Loop over folds\n",
    "    for fold_ix in range(n_folds) :\n",
    "\n",
    "        yh = models[fold_ix](sequence_one_hot[None, ...])[:, None, ...].astype('float32')\n",
    "        \n",
    "        #Undo scale\n",
    "        yh /= track_scale\n",
    "\n",
    "        #Undo soft_clip\n",
    "        if clip_soft is not None :\n",
    "            yh_unclipped = (yh - clip_soft)**2 + clip_soft\n",
    "            unclip_mask_h = (yh > clip_soft)\n",
    "\n",
    "            yh[unclip_mask_h] = yh_unclipped[unclip_mask_h]\n",
    "\n",
    "        #Undo sqrt\n",
    "        yh = yh**(1. / track_transform)\n",
    "\n",
    "        predicted_tracks.append(yh)\n",
    "\n",
    "    #Concatenate\n",
    "    predicted_tracks = np.concatenate(predicted_tracks, axis=1)\n",
    "    print(\"predicted_tracks.shape = \" + str(predicted_tracks.shape))\n",
    "    \n",
    "    return predicted_tracks\n",
    "\n",
    "#Helper function to get (padded) one-hot and annotated sub-dataframe\n",
    "def process_sequence(chrom, start, end) :\n",
    "    \n",
    "    seq_len = end - start\n",
    "\n",
    "    #Pad sequence to input window size\n",
    "    start -= (SEQUENCE_LENGTH - seq_len) // 2\n",
    "    end += (SEQUENCE_LENGTH - seq_len) // 2\n",
    "\n",
    "    annotation_df = gene_pr.df.query(\"Chromosome == '\" + chrom + \"' and ((End >= \" + str(int(start)) + \" and End < \" + str(int(end)) + \") or (Start >= \" + str(int(start)) + \" and Start < \" + str(int(end)) + \"))\")\n",
    "\n",
    "    #Get one-hot\n",
    "    sequence_one_hot = make_seq_1hot(fasta_open, chrom, start, end, seq_len)\n",
    "    \n",
    "    return sequence_one_hot.astype('float32'), annotation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "892f7fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize fasta sequence extractor\n",
    "\n",
    "fasta_file = 'hg38.fa'\n",
    "\n",
    "#!wget -O - http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz | gunzip -c > {fasta_file}\n",
    "#pyfaidx.Faidx(fasta_file)\n",
    "\n",
    "fasta_open = pysam.Fastafile(fasta_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f94ec49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(fold_dfs[0]) = 1430\n"
     ]
    }
   ],
   "source": [
    "#Load sequences.bed test sets for each model fold\n",
    "\n",
    "fold_dfs = []\n",
    "for fold_ix in range(n_folds) :\n",
    "    \n",
    "    df_bed = pd.read_csv('/home/drk/seqnn/data/models/v2/f0c0/data0/sequences.bed', names=['chrom', 'start', 'end', 'train_or_test'], sep='\\t')\n",
    "    \n",
    "    test_df = df_bed.query(\"train_or_test == 'test'\").copy().reset_index(drop=True)\n",
    "    \n",
    "    #Greedily keep only non-overlapping coordinate ranges\n",
    "    keep_index = []\n",
    "    \n",
    "    for row_i, [_, row] in enumerate(test_df.iterrows()) :\n",
    "        test_df_chrom = test_df.iloc[keep_index].query(\"chrom == '\" + row['chrom'] + \"'\")\n",
    "        \n",
    "        if len(test_df_chrom.query(\"(\" + str(row['start']) + \" >= start and \" + str(row['start']) + \" < end) or (\" + str(row['end']) + \" >= start and \" + str(row['end']) + \" < end)\")) <= 0 :\n",
    "            keep_index.append(row_i)\n",
    "    \n",
    "    fold_dfs.append(test_df.iloc[keep_index].copy().reset_index(drop=True))\n",
    "\n",
    "print(\"len(fold_dfs[0]) = \" + str(len(fold_dfs[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28113177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(peak_dfs[0]) = 35323\n"
     ]
    }
   ],
   "source": [
    "#Load h3k4me4 peaks for a few representative tissues\n",
    "\n",
    "min_q = 0.05\n",
    "\n",
    "peak_dfs = []\n",
    "\n",
    "files = [\n",
    "    'ENCFF266XUO_GSM669925_H3K4me3_Adipose.bed',\n",
    "    'ENCFF416SIL_GSM621675_H3K4me3_Liver.bed',\n",
    "    'ENCFF726NBW_GSM621694_H3K4me3_Muscle.bed',\n",
    "]\n",
    "\n",
    "#Load and merge peaks\n",
    "for file in files :\n",
    "    \n",
    "    peak_df = pd.read_csv(file, names=['chrom', 'start', 'end', 'peak_name', 'peak_score', 'strand', 'signal_value', 'pval', 'qval', 'offset'], sep='\\t')\n",
    "    \n",
    "    signal_vals = np.array(peak_df['signal_value'].values)\n",
    "    \n",
    "    #Get quantile and filter on signal value\n",
    "    min_q_val = np.quantile(signal_vals, q=min_q)\n",
    "    peak_df = peak_df.query(\"signal_value >= \" + str(min_q_val)).copy().reset_index(drop=True)\n",
    "    \n",
    "    peak_dfs.append(peak_df)\n",
    "\n",
    "print(\"len(peak_dfs[0]) = \" + str(len(peak_dfs[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9671441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- test_ix = 0 ---\n",
      "WARNING:tensorflow:From /home/jlinder/anaconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 16:43:08.505362: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-08 16:43:09.063173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10398 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 16:43:20.615340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8600\n",
      "2023-08-08 16:43:20.929661: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 693ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 573ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 747ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 935ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f740bb7e550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7401423ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 1 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 429ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 744ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 908ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 2 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 433ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 573ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 743ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 909ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 3 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 428ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 563ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 903ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 4 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 732ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 903ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 5 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 429ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 565ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 731ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 902ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 6 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 422ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 730ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 902ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 7 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 429ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 899ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 8 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 583ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 770ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 935ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 9 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 579ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 750ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 928ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 10 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 445ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 576ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 753ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 931ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 11 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 436ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 570ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 743ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 917ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 12 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 439ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 575ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 745ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 911ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 13 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 438ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 572ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 790ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 910ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 14 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 430ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 587ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 908ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 15 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 385ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 565ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 904ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 16 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 433ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 582ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 755ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 922ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 17 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 454ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 584ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 747ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 934ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 18 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 450ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 576ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 750ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 921ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 19 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 429ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 742ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 908ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 20 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 571ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 902ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 21 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 433ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 893ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 22 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 433ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 563ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 730ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 894ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 23 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 423ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 895ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 24 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 423ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 575ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 746ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 918ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 25 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 449ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 575ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 747ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 933ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 26 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 571ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 750ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 924ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 27 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 569ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 743ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 909ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 28 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 904ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 29 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 440ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 900ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 30 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 565ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 753ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 912ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 31 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 427ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 734ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 899ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 32 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 425ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 576ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 747ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 920ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 33 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 573ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 749ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 935ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 34 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 442ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 741ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 913ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 35 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 439ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 575ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 741ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 912ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 36 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 436ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 737ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 910ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 37 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 739ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 906ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 38 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 422ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 562ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 732ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 902ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 39 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 427ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 571ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 908ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 40 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 423ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 574ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 747ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 919ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 41 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 444ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 580ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 745ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 920ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 42 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 740ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 913ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 43 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 443ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 574ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 740ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 913ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 44 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 734ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 908ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 45 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 739ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 900ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 46 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 427ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 562ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 730ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 898ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 47 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 433ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 576ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 900ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 48 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 419ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 587ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 750ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 919ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 49 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 449ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 580ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 751ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 934ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 50 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 441ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 573ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 742ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 920ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 51 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 445ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 573ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 740ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 914ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 52 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 441ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 569ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 737ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 905ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 53 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 425ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 565ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 898ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 54 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 426ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 563ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 900ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 55 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 429ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 563ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 900ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 56 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 426ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 582ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 750ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 922ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 57 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 457ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 577ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 751ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 926ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 58 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 443ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 577ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 746ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 920ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 59 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 436ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 745ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 915ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 60 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 737ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 919ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 752ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 933ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 106 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 436ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 621ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 741ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 914ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 107 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 426ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 570ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 741ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 914ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 108 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 441ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 569ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 909ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 109 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 565ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 908ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 110 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 563ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 903ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 111 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 734ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 903ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 112 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 430ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 581ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 745ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 915ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 113 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 457ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 576ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 748ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 923ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 114 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 572ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 742ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 916ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 115 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 444ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 738ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 911ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 116 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 436ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 906ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 117 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 440ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 734ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 903ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 118 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 563ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 898ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 119 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 900ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 120 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 424ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 581ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 758ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 918ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 121 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 459ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 577ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 749ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 918ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 122 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 443ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 573ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 743ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 915ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 123 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 438ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 572ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 740ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 911ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 124 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 905ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 125 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 732ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 898ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 126 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 562ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 901ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 127 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 427ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 732ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 898ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 128 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 424ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 578ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 753ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 919ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 129 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 451ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 577ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 746ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 922ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 130 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 449ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 572ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 743ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 916ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 131 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 438ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 741ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 912ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 132 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 436ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 570ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 747ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 912ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 133 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 428ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 562ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 732ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 902ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 134 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 438ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 900ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 135 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 438ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 565ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 734ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 900ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 136 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 742ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 914ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 164 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 427ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 738ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 912ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 165 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 429ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 565ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 734ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 902ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 166 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 565ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 902ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 167 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 427ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 565ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 737ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 904ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 168 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 428ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 578ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 750ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 928ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 169 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 453ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 579ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 749ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 930ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 170 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 483ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 744ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 914ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 171 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 569ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 742ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 909ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 172 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 425ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 740ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 906ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 173 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 429ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 562ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 902ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 174 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 449ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 731ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 897ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 175 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 571ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 734ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 906ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 176 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 420ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 577ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 747ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 930ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 177 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 445ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 577ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 748ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 921ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 178 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 574ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 753ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 915ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 179 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 428ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 741ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 914ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 180 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 496ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 572ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 742ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 911ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 181 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 436ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 907ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 182 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 428ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 565ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 902ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 183 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 434ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 563ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 901ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 184 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 423ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 583ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 746ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 923ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 185 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 457ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 582ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 750ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 940ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 186 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 447ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 575ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 744ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 917ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 187 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 444ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 570ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 741ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 912ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 188 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 440ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 905ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 189 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 570ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 739ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 903ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 190 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 436ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 738ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 909ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 191 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 427ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 741ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 902ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 192 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 582ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 751ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 922ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 193 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 450ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 580ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 751ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 926ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 194 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 441ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 575ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 752ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 917ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 195 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 570ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 742ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 914ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 196 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 740ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 910ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 197 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 433ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 734ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 905ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 198 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 436ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 901ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 199 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 569ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 903ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 200 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 430ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 584ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 753ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 921ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 201 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 455ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 582ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 750ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 935ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 202 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 445ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 573ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 748ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 922ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 203 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 450ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 573ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 744ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 911ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 204 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 438ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 782ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 905ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 205 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 444ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 901ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 206 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 433ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 902ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 207 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 906ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 208 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 430ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 581ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 761ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 924ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 209 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 490ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 578ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 746ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 918ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 210 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 617ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 754ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 919ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 211 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 801ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 912ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 212 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 426ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 573ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 737ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 988ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 213 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 428ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 904ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 214 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 428ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 565ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 734ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 905ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 215 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 433ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 732ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 901ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 216 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 430ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 581ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 748ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 924ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 217 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 441ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 587ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 750ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 939ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 218 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 449ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 573ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 746ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 913ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 219 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 443ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 740ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 912ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 220 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 508ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 571ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 737ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 906ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 221 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 440ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 911ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 222 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 563ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 823ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 909ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 223 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 930ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 224 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 502ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 579ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 750ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 922ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 225 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 461ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 582ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 752ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 922ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 226 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 445ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 580ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 809ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 925ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 227 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 438ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 573ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 742ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 911ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 228 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 433ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 904ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 229 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 443ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 902ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 230 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 565ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 903ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 231 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 438ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 565ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 734ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 899ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 232 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 440ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 584ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 751ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 932ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 233 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 455ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 581ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 749ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 921ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 234 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 455ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 577ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 748ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 916ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 235 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 515ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 738ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 912ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 236 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 448ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 642ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 906ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 237 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 813ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 902ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 238 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 430ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 563ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 821ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 904ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 239 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 424ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 563ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 965ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 240 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 539ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 585ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 755ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 923ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 241 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 453ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 660ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 753ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 930ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 242 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 442ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 574ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 787ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 923ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 243 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 434ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 571ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 742ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 995ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 244 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 444ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 905ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 245 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 433ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 735ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 897ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 246 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 428ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 903ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 247 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 564ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 733ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 903ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 248 ---\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 524288, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 524288, 4),  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 524288, 4)   0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 524288, 512)  31232       ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 262144, 512)  0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " sync_batch_normalization (Sync  (None, 262144, 512)  2048       ['max_pooling1d[0][0]']          \n",
      " BatchNormalization)                                                                              \n",
      "                                                                                                  \n",
      " tf.nn.gelu (TFOpLambda)        (None, 262144, 512)  0           ['sync_batch_normalization[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 262144, 608)  1557088     ['tf.nn.gelu[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 131072, 608)  0          ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_1 (Sy  (None, 131072, 608)  2432       ['max_pooling1d_1[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_1 (TFOpLambda)      (None, 131072, 608)  0           ['sync_batch_normalization_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 131072, 736)  2238176     ['tf.nn.gelu_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 65536, 736)  0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_2 (Sy  (None, 65536, 736)  2944        ['max_pooling1d_2[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_2 (TFOpLambda)      (None, 65536, 736)   0           ['sync_batch_normalization_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 65536, 896)   3298176     ['tf.nn.gelu_2[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 32768, 896)  0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_3 (Sy  (None, 32768, 896)  3584        ['max_pooling1d_3[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_3 (TFOpLambda)      (None, 32768, 896)   0           ['sync_batch_normalization_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 32768, 1056)  4731936     ['tf.nn.gelu_3[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 16384, 1056)  0          ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_4 (Sy  (None, 16384, 1056)  4224       ['max_pooling1d_4[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_4 (TFOpLambda)      (None, 16384, 1056)  0           ['sync_batch_normalization_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 16384, 1280)  6759680     ['tf.nn.gelu_4[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 8192, 1280)  0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " sync_batch_normalization_5 (Sy  (None, 8192, 1280)  5120        ['max_pooling1d_5[0][0]']        \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_5 (TFOpLambda)      (None, 8192, 1280)   0           ['sync_batch_normalization_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 8192, 1536)   9831936     ['tf.nn.gelu_5[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 4096, 1536)  0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 4096, 1536)  3072        ['max_pooling1d_6[0][0]']        \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multihead_attention (Multihead  (None, 4096, 1536)  6310400     ['layer_normalization[0][0]']    \n",
      " Attention)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096, 1536)   0           ['multihead_attention[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 4096, 1536)   0           ['max_pooling1d_6[0][0]',        \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 4096, 1536)  3072        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096, 3072)   4721664     ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 4096, 3072)   0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4096, 3072)   0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4096, 1536)   4720128     ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4096, 1536)   0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 4096, 1536)   0           ['add[0][0]',                    \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 4096, 1536)  3072        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_1 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_2[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 4096, 1536)   0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 4096, 1536)  3072        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 4096, 3072)   0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 4096, 3072)   0           ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 4096, 1536)   0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 4096, 1536)   0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 4096, 1536)  3072        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_2 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_4[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 4096, 1536)   0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 4096, 1536)  3072        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 4096, 3072)   0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 4096, 3072)   0           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4096, 1536)   0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 4096, 1536)   0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 4096, 1536)  3072        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_3 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_6[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 4096, 1536)   0           ['multihead_attention_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 4096, 1536)   0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 4096, 1536)  3072        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 4096, 3072)   0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 4096, 3072)   0           ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 4096, 1536)   0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 4096, 1536)   0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4096, 1536)  3072        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multihead_attention_4 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_8[0][0]']  \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 4096, 1536)   0           ['add_7[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 4096, 1536)  3072        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4096, 3072)   4721664     ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 4096, 3072)   0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 4096, 3072)   0           ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 4096, 1536)   4720128     ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 4096, 1536)   0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 4096, 1536)   0           ['add_8[0][0]',                  \n",
      "                                                                  'dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 4096, 1536)  3072        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_5 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_10[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 4096, 1536)   0           ['add_9[0][0]',                  \n",
      "                                                                  'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 4096, 1536)  3072        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 4096, 3072)   0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 4096, 3072)   0           ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 4096, 1536)   0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 4096, 1536)   0           ['add_10[0][0]',                 \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 4096, 1536)  3072        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_6 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_12[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 4096, 1536)   0           ['add_11[0][0]',                 \n",
      "                                                                  'dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 4096, 1536)  3072        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 4096, 3072)   0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 4096, 3072)   0           ['dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 4096, 1536)   0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 4096, 1536)   0           ['add_12[0][0]',                 \n",
      "                                                                  'dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 4096, 1536)  3072        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multihead_attention_7 (Multihe  (None, 4096, 1536)  6310400     ['layer_normalization_14[0][0]'] \n",
      " adAttention)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 4096, 1536)   0           ['multihead_attention_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4096, 1536)   0           ['add_13[0][0]',                 \n",
      "                                                                  'dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 4096, 1536)  3072        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 4096, 3072)   4721664     ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 4096, 3072)   0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 4096, 3072)   0           ['dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 4096, 1536)   4720128     ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 4096, 1536)   0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 4096, 1536)   0           ['add_14[0][0]',                 \n",
      "                                                                  'dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_6 (Sy  (None, 4096, 1536)  6144        ['add_15[0][0]']                 \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_6 (TFOpLambda)      (None, 4096, 1536)   0           ['sync_batch_normalization_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_7 (Sy  (None, 8192, 1536)  6144        ['conv1d_6[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 4096, 1536)   2360832     ['tf.nn.gelu_6[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_7 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_7[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 8192, 1536)   0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8192, 1536)   0           ['up_sampling1d[0][0]',          \n",
      "                                                                  'dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d (SeparableCon  (None, 8192, 1536)  2365440     ['add_16[0][0]']                 \n",
      " v1D)                                                                                             \n",
      "                                                                                                  \n",
      " sync_batch_normalization_8 (Sy  (None, 8192, 1536)  6144        ['separable_conv1d[0][0]']       \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.gelu_8 (TFOpLambda)      (None, 8192, 1536)   0           ['sync_batch_normalization_8[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " sync_batch_normalization_9 (Sy  (None, 16384, 1280)  5120       ['conv1d_5[0][0]']               \n",
      " ncBatchNormalization)                                                                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 8192, 1536)   2360832     ['tf.nn.gelu_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_9 (TFOpLambda)      (None, 16384, 1280)  0           ['sync_batch_normalization_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 16384, 1536)  0          ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 16384, 1536)  1967616     ['tf.nn.gelu_9[0][0]']           \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 16384, 1536)  0           ['up_sampling1d_1[0][0]',        \n",
      "                                                                  'dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " separable_conv1d_1 (SeparableC  (None, 16384, 1536)  2365440    ['add_17[0][0]']                 \n",
      " onv1D)                                                                                           \n",
      "                                                                                                  \n",
      " cropping1d (Cropping1D)        (None, 16352, 1536)  0           ['separable_conv1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " sync_batch_normalization_10 (S  (None, 16352, 1536)  6144       ['cropping1d[0][0]']             \n",
      " yncBatchNormalization)                                                                           \n",
      "                                                                                                  \n",
      " tf.nn.gelu_10 (TFOpLambda)     (None, 16352, 1536)  0           ['sync_batch_normalization_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 16352, 1920)  2951040     ['tf.nn.gelu_10[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 16352, 1920)  0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.nn.gelu_11 (TFOpLambda)     (None, 16352, 1920)  0           ['dropout_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 16352, 7611)  14620731    ['tf.nn.gelu_11[0][0]']          \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 16352, 7611)  0          ['dense_20[0][0]',               \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 185,917,723\n",
      "Trainable params: 185,892,699\n",
      "Non-trainable params: 25,024\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [32, 32]\n",
      "target_lengths [16352, 16352]\n",
      "target_crops [16, 16]\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 581ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 750ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 930ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 249 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 446ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 581ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 749ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 984ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 250 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 440ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 582ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 746ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 914ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 251 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 442ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 569ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 740ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 927ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 252 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 433ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 575ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 737ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 907ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 253 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 568ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 739ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 914ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 254 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 528ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 739ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 908ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      " --- test_ix = 255 ---\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 0s 423ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 674ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 736ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 900ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'r_w_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'r_r_bias:0' shape=(1, 8, 1, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "predicted_tracks.shape = (1, 1, 16352, 89)\n",
      "attention_scores.shape = (1, 1, 8, 8, 4096, 4096)\n",
      "\n",
      "peak_i = 0\n",
      "peak_atts[peak_i].shape = (1570,)\n",
      "peak_atts_neg[peak_i].shape = (6280,)\n",
      "\n",
      "peak_i = 1\n",
      "peak_atts[peak_i].shape = (1854,)\n",
      "peak_atts_neg[peak_i].shape = (7416,)\n",
      "\n",
      "peak_i = 2\n",
      "peak_atts[peak_i].shape = (2206,)\n",
      "peak_atts_neg[peak_i].shape = (8824,)\n",
      "\n",
      "(splice)\n",
      "splice_atts.shape = (13919,)\n",
      "splice_atts_neg.shape = (55676,)\n",
      "\n",
      "(pA)\n",
      "pa_atts.shape = (1919,)\n",
      "pa_atts_neg.shape = (7676,)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "fold_ix = 0\n",
    "n_test = 256\n",
    "\n",
    "negative_frac = 4\n",
    "\n",
    "peak_atts = [[] for peak_i in range(len(peak_dfs))]\n",
    "peak_atts_neg = [[] for peak_i in range(len(peak_dfs))]\n",
    "\n",
    "splice_atts = []\n",
    "splice_atts_neg = []\n",
    "\n",
    "pa_atts = []\n",
    "pa_atts_neg = []\n",
    "\n",
    "#Loop over test sequences\n",
    "for test_ix in range(n_test) :\n",
    "    \n",
    "    print(\" --- test_ix = \" + str(test_ix) + \" ---\")\n",
    "    \n",
    "    #Get test sequence coordinates\n",
    "    test_row = fold_dfs[fold_ix].iloc[test_ix]\n",
    "    chrom, start, end = test_row['chrom'], test_row['start'], test_row['end']\n",
    "    \n",
    "    start -= (524288 - 196608) // 2\n",
    "    end += (524288 - 196608) // 2\n",
    "    \n",
    "    #Get test sequence\n",
    "    sequence_one_hot, annotation_df = process_sequence(chrom, start, end)\n",
    "\n",
    "    #Potentially re-initialize model ensemble (to deal with memory fragmentation issues)\n",
    "    if test_ix % 8 == 0 :\n",
    "        K.clear_session()\n",
    "\n",
    "        models = []\n",
    "        model_file = \"../bench_apa/test_apa/f\" + str(fold_ix) + \"c0/train/model0_best.h5\"\n",
    "\n",
    "        seqnn_model = seqnn.SeqNN(params_model)\n",
    "        seqnn_model.restore(model_file, 0, by_name=False)\n",
    "        seqnn_model.build_slice(target_index)\n",
    "        seqnn_model.build_ensemble(True, '0')\n",
    "\n",
    "        models.append(seqnn_model)\n",
    "\n",
    "    #Predict attention scores\n",
    "    _, attention_scores = predict_tracks_and_attention_scores(models, sequence_one_hot, score_rc=True)\n",
    "\n",
    "    fold_index = [0]\n",
    "    layer_index = [5, 6]\n",
    "    head_index = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "    att = np.mean(\n",
    "        np.mean(\n",
    "            np.mean(\n",
    "                attention_scores[:, fold_index, ...]\n",
    "                , axis=1\n",
    "            )[:, layer_index, ...]\n",
    "            , axis=1\n",
    "        )[:, head_index, ...]\n",
    "        , axis=1\n",
    "    )[0, ...]\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    #Extract attention bin positions from regions overlapping H3K4me3 peaks\n",
    "    peak_bins = []\n",
    "    \n",
    "    for peak_i in range(len(peak_dfs)) :\n",
    "        \n",
    "        peak_df = peak_dfs[peak_i].query(\"chrom == '\" + chrom + \"' and start + offset >= \" + str(start) + \" and start + offset < \" + str(end))\n",
    "        peak_bins.append([])\n",
    "\n",
    "        #Loop over annotated peaks\n",
    "        for _, peak_row in peak_df.iterrows() :\n",
    "            att_bin_start = max(int(np.round((peak_row['start'] - start) / 128.)), 0)\n",
    "            att_bin_end = min(int(np.round((peak_row['end'] - start) / 128.) + 1), 4096)\n",
    "\n",
    "            if att_bin_end > att_bin_start :\n",
    "                peak_bins[peak_i].append(np.arange(att_bin_start, att_bin_end, dtype='int32').tolist())\n",
    "\n",
    "    #Extract attention bin positions from exons or regions overlapping pA sites\n",
    "    splice_df = annotation_df.query(\"Feature == 'exon' and gene_type == 'protein_coding'\").drop_duplicates(subset=['exon_id'], keep='first').drop_duplicates(subset=['Chromosome', 'Start', 'End'], keep='first').copy().reset_index(drop=True)[['Chromosome', 'Start', 'End']]\n",
    "    pa_df = annotation_df.query(\"Feature == 'polyA_signal'\").drop_duplicates(subset=['Chromosome', 'Start', 'End'], keep='first').copy().reset_index(drop=True)[['Chromosome', 'Start', 'End']]\n",
    "    \n",
    "    splice_bins = []\n",
    "    \n",
    "    #Loop over annotated splice junctions\n",
    "    for _, splice_row in splice_df.iterrows() :\n",
    "        att_bin_start = max(int(np.round((splice_row['Start'] - start) / 128.)), 0)\n",
    "        att_bin_end = min(int(np.round((splice_row['End'] - start) / 128.) + 1), 4096)\n",
    "\n",
    "        if att_bin_end > att_bin_start :\n",
    "            splice_bins.append(np.arange(att_bin_start, att_bin_end, dtype='int32').tolist())\n",
    "    \n",
    "    pa_bins = []\n",
    "    #Loop over annotated polyA sites\n",
    "    for _, pa_row in pa_df.iterrows() :\n",
    "        att_bin_start = max(int(np.round((pa_row['Start'] - start - 64) / 128.)), 0)\n",
    "        att_bin_end = min(int(np.round((pa_row['End'] - start + 64) / 128.) + 1), 4096)\n",
    "\n",
    "        if att_bin_end > att_bin_start :\n",
    "            pa_bins.append(np.arange(att_bin_start, att_bin_end, dtype='int32').tolist())\n",
    "    \n",
    "    #Concatenate all non-background bins\n",
    "    all_bins = splice_bins + pa_bins\n",
    "    for peak_i in range(len(peak_bins)) :\n",
    "        all_bins = all_bins + peak_bins[peak_i]\n",
    "    \n",
    "    if len(all_bins) > 0 :\n",
    "        all_bin_set = set(np.concatenate([np.array(bin_set) for bin_set in all_bins], axis=0).tolist())\n",
    "\n",
    "        #Construct negative bins\n",
    "        negative_bins = [bin_ix for bin_ix in np.arange(4096).tolist() if bin_ix not in all_bin_set]\n",
    "\n",
    "        #Construct matched negative sets and accumulate across test set (promoters)\n",
    "        for peak_i in range(len(peak_bins)) :\n",
    "            \n",
    "            if len(peak_bins[peak_i]) > 0 :\n",
    "                \n",
    "                peak_bins_neg = []\n",
    "                \n",
    "                #Loop over bins and sample negatives\n",
    "                for bin_set in peak_bins[peak_i] :\n",
    "                    for negative_frac_ix in range(negative_frac) :\n",
    "                        peak_bins_neg.append(np.random.choice(negative_bins, size=(len(bin_set),), replace=True).tolist())\n",
    "\n",
    "                peak_att = np.array([np.mean(att[:, peak_bins[peak_i][bin_set_ix]]) for bin_set_ix in range(len(peak_bins[peak_i]))], dtype='float32')\n",
    "                peak_att_neg = np.array([np.mean(att[:, peak_bins_neg[bin_set_ix]]) for bin_set_ix in range(len(peak_bins_neg))], dtype='float32')\n",
    "\n",
    "                peak_atts[peak_i].append(peak_att)\n",
    "                peak_atts_neg[peak_i].append(peak_att_neg)\n",
    "        \n",
    "        #Matched splice set\n",
    "        if len(splice_bins) > 0 :\n",
    "                \n",
    "            splice_bins_neg = []\n",
    "\n",
    "            #Loop over bins and sample negatives\n",
    "            for bin_set in splice_bins :\n",
    "                for negative_frac_ix in range(negative_frac) :\n",
    "                    splice_bins_neg.append(np.random.choice(negative_bins, size=(len(bin_set),), replace=True).tolist())\n",
    "\n",
    "            splice_att = np.array([np.mean(att[:, splice_bins[bin_set_ix]]) for bin_set_ix in range(len(splice_bins))], dtype='float32')\n",
    "            splice_att_neg = np.array([np.mean(att[:, splice_bins_neg[bin_set_ix]]) for bin_set_ix in range(len(splice_bins_neg))], dtype='float32')\n",
    "\n",
    "            splice_atts.append(splice_att)\n",
    "            splice_atts_neg.append(splice_att_neg)\n",
    "        \n",
    "        #Matched pA set\n",
    "        if len(pa_bins) > 0 :\n",
    "                \n",
    "            pa_bins_neg = []\n",
    "\n",
    "            #Loop over bins and sample negatives\n",
    "            for bin_set in pa_bins :\n",
    "                for negative_frac_ix in range(negative_frac) :\n",
    "                    pa_bins_neg.append(np.random.choice(negative_bins, size=(len(bin_set),), replace=True).tolist())\n",
    "\n",
    "            pa_att = np.array([np.mean(att[:, pa_bins[bin_set_ix]]) for bin_set_ix in range(len(pa_bins))], dtype='float32')\n",
    "            pa_att_neg = np.array([np.mean(att[:, pa_bins_neg[bin_set_ix]]) for bin_set_ix in range(len(pa_bins_neg))], dtype='float32')\n",
    "\n",
    "            pa_atts.append(pa_att)\n",
    "            pa_atts_neg.append(pa_att_neg)\n",
    "\n",
    "#Loop over peak tissue types and concatenate scores\n",
    "for peak_i in range(len(peak_bins)) :\n",
    "    print(\"\")\n",
    "    print(\"peak_i = \" + str(peak_i))\n",
    "    \n",
    "    peak_atts[peak_i] = np.concatenate(peak_atts[peak_i], axis=0)\n",
    "    peak_atts_neg[peak_i] = np.concatenate(peak_atts_neg[peak_i], axis=0)\n",
    "\n",
    "    print(\"peak_atts[peak_i].shape = \" + str(peak_atts[peak_i].shape))\n",
    "    print(\"peak_atts_neg[peak_i].shape = \" + str(peak_atts_neg[peak_i].shape))\n",
    "\n",
    "print(\"\")\n",
    "print(\"(splice)\")\n",
    "\n",
    "#Concatenate splice scores\n",
    "splice_atts = np.concatenate(splice_atts, axis=0)\n",
    "splice_atts_neg = np.concatenate(splice_atts_neg, axis=0)\n",
    "\n",
    "print(\"splice_atts.shape = \" + str(splice_atts.shape))\n",
    "print(\"splice_atts_neg.shape = \" + str(splice_atts_neg.shape))\n",
    "\n",
    "print(\"\")\n",
    "print(\"(pA)\")\n",
    "\n",
    "#Concatenate polyA scores\n",
    "pa_atts = np.concatenate(pa_atts, axis=0)\n",
    "pa_atts_neg = np.concatenate(pa_atts_neg, axis=0)\n",
    "\n",
    "print(\"pa_atts.shape = \" + str(pa_atts.shape))\n",
    "print(\"pa_atts_neg.shape = \" + str(pa_atts_neg.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93a9a323",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- H3K4me3 -\n",
      "--- GSM669925_Adipose ---\n",
      " - n (pos) = 1570\n",
      " - n (neg) = 6280\n",
      " -- p = 0.0\n",
      " -- s = 52.9357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GSM621675_Liver ---\n",
      " - n (pos) = 1854\n",
      " - n (neg) = 7416\n",
      " -- p = 0.0\n",
      " -- s = 42.6267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GSM621694_Muscle ---\n",
      " - n (pos) = 2206\n",
      " - n (neg) = 8824\n",
      " -- p = 0.0\n",
      " -- s = 54.3695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize enrichment of attention overlapping H3K4me3 peaks\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "from scipy.stats import ranksums\n",
    "\n",
    "save_figs = True\n",
    "\n",
    "print(\"- H3K4me3 -\")\n",
    "\n",
    "peak_names = [\n",
    "    'GSM669925_Adipose',\n",
    "    'GSM621675_Liver',\n",
    "    'GSM621694_Muscle',\n",
    "]\n",
    "\n",
    "#Loop over peak sets (tissues)\n",
    "for peak_i in range(len(peak_bins)) :\n",
    "    \n",
    "    print(\"--- \" + peak_names[peak_i] + \" ---\")\n",
    "    \n",
    "    peak_att = peak_atts[peak_i]\n",
    "    peak_att_neg = peak_atts_neg[peak_i]\n",
    "    \n",
    "    #Compute two-sided ranksum test\n",
    "    s_val, p_val = ranksums(peak_att, peak_att_neg, alternative='two-sided')\n",
    "    \n",
    "    print(\" - n (pos) = \" + str(peak_att.shape[0]))\n",
    "    print(\" - n (neg) = \" + str(peak_att_neg.shape[0]))\n",
    "    print(\" -- p = \" + str(p_val))\n",
    "    print(\" -- s = \" + str(round(s_val, 4)))\n",
    "\n",
    "    median_att = np.median(peak_att)\n",
    "    median_att_neg = np.median(peak_att_neg)\n",
    "    \n",
    "    f = plt.figure(figsize=(3, 3))\n",
    "\n",
    "    sns.stripplot(data=[peak_att, peak_att_neg], s=4, palette=['lightcoral', 'lightgreen'], alpha=0.9, jitter=0.2)\n",
    "    \n",
    "    plt.plot([-0.5, 0.5], [median_att, median_att], color='black', linestyle='--', linewidth=2)\n",
    "    plt.plot([ 0.5, 1.5], [median_att_neg, median_att_neg], color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "    plt.title(peak_names[peak_i] + \"\\np < \" + '{:.2e}'.format(p_val), fontsize=8)\n",
    "    \n",
    "    plt.xticks([0, 1], [\"Peak\", \"Background\"], fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    \n",
    "    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.4f'))\n",
    "    \n",
    "    plt.ylabel(\"Attention Value\", fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_figs :\n",
    "        plt.savefig(\"borzoi_v2_attention_h3k4me3_\" + str(peak_names[peak_i]) + \"_\" + str(n_folds) + \"_folds.png\", transparent=False)\n",
    "        plt.savefig(\"borzoi_v2_attention_h3k4me3_\" + str(peak_names[peak_i]) + \"_\" + str(n_folds) + \"_folds.eps\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dacffcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Exons -\n",
      " - n (pos) = 13919\n",
      " - n (neg) = 55676\n",
      " -- p = 0.0\n",
      " -- s = 159.1133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize enrichment of attention overlapping exons\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "from scipy.stats import ranksums\n",
    "\n",
    "save_figs = True\n",
    "\n",
    "print(\"- Exons -\")\n",
    "\n",
    "#Compute two-sided ranksum test\n",
    "s_val, p_val = ranksums(splice_atts, splice_atts_neg, alternative='two-sided')\n",
    "\n",
    "print(\" - n (pos) = \" + str(splice_atts.shape[0]))\n",
    "print(\" - n (neg) = \" + str(splice_atts_neg.shape[0]))\n",
    "print(\" -- p = \" + str(p_val))\n",
    "print(\" -- s = \" + str(round(s_val, 4)))\n",
    "\n",
    "median_att = np.median(splice_atts)\n",
    "median_att_neg = np.median(splice_atts_neg)\n",
    "\n",
    "f = plt.figure(figsize=(3, 3))\n",
    "\n",
    "sns.stripplot(data=[splice_atts, splice_atts_neg], s=4, palette=['lightcoral', 'lightgreen'], alpha=0.9, jitter=0.2)\n",
    "\n",
    "plt.plot([-0.5, 0.5], [median_att, median_att], color='black', linestyle='--', linewidth=2)\n",
    "plt.plot([ 0.5, 1.5], [median_att_neg, median_att_neg], color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.title(\"GENCODE 41\\np < \" + '{:.2e}'.format(p_val), fontsize=8)\n",
    "\n",
    "plt.xticks([0, 1], [\"Exon\", \"Background\"], fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.4f'))\n",
    "\n",
    "plt.ylabel(\"Attention Value\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_figs :\n",
    "    plt.savefig(\"borzoi_v2_attention_exons_\" + str(n_folds) + \"_folds.png\", transparent=False)\n",
    "    plt.savefig(\"borzoi_v2_attention_exons_\" + str(n_folds) + \"_folds.eps\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cf5c346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- pA sites -\n",
      " - n (pos) = 1919\n",
      " - n (neg) = 7676\n",
      " -- p = 0.0\n",
      " -- s = 54.1067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize enrichment of attention overlapping pA sites\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "from scipy.stats import ranksums\n",
    "\n",
    "save_figs = True\n",
    "\n",
    "print(\"- pA sites -\")\n",
    "\n",
    "#Compute two-sided ranksum test\n",
    "s_val, p_val = ranksums(pa_atts, pa_atts_neg, alternative='two-sided')\n",
    "\n",
    "print(\" - n (pos) = \" + str(pa_atts.shape[0]))\n",
    "print(\" - n (neg) = \" + str(pa_atts_neg.shape[0]))\n",
    "print(\" -- p = \" + str(p_val))\n",
    "print(\" -- s = \" + str(round(s_val, 4)))\n",
    "\n",
    "median_att = np.median(pa_atts)\n",
    "median_att_neg = np.median(pa_atts_neg)\n",
    "\n",
    "f = plt.figure(figsize=(3, 3))\n",
    "\n",
    "sns.stripplot(data=[pa_atts, pa_atts_neg], s=4, palette=['lightcoral', 'lightgreen'], alpha=0.9, jitter=0.2)\n",
    "\n",
    "plt.plot([-0.5, 0.5], [median_att, median_att], color='black', linestyle='--', linewidth=2)\n",
    "plt.plot([ 0.5, 1.5], [median_att_neg, median_att_neg], color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.title(\"GENCODE 41\\np < \" + '{:.2e}'.format(p_val), fontsize=8)\n",
    "\n",
    "plt.xticks([0, 1], [\"pA Signal\", \"Background\"], fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.4f'))\n",
    "\n",
    "plt.ylabel(\"Attention Value\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_figs :\n",
    "    plt.savefig(\"borzoi_v2_attention_pa_\" + str(n_folds) + \"_folds.png\", transparent=False)\n",
    "    plt.savefig(\"borzoi_v2_attention_pa_\" + str(n_folds) + \"_folds.eps\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a5f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
